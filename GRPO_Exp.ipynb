{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "aa0535e44df0458baac6a7698b9fab44",
      "0ee353103bdc41d58e3b109b6d32b981",
      "e8df8706f65c4e058ec6a997fb217909",
      "20c516894abe4ac99ec6394c10e8e7b7",
      "3a6425182f7148108bdf3d0d83e2306b",
      "53dd155e24c34044831a0c55933139aa",
      "6c7d20eaa3344a09a7aabcfb4138f2cf",
      "35016e98dd0b4dc8a63b81505c3af282",
      "76ff1098d62b41eb90d092f0728b25d3",
      "a9373dd92cff41ea86495068b9c6c353",
      "35012e1d377d43fb83d41fa9ff30ef98",
      "e0a689b7c40a4d4b9a1e66e77a44eddf",
      "3e9b35d3af4e48a697ec4cab68919cf1",
      "6bc401ce58f34f8fa949ef30dce2ee18",
      "5654dfed63da4f5eaef8c818fc4a2beb",
      "6abf7ae43cfe41fca75777aff3e0a2e9",
      "3b34928d0aa945b482bb162cc992a5bc",
      "97a068369d8a4f6a956feea29e50e4b4",
      "0a35311ba64e4b4ea7b1493c63f93a1f",
      "1c878ab261b34eeda276fd59b84b9219"
     ]
    },
    "id": "59UCvPSNXuaw",
    "outputId": "eedb6ba6-8da4-4663-db0d-8e7581b7ce15"
   },
   "outputs": [],
   "source": [
    "!pip install -q huggingface_hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSEdMNmPkf-z"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    !pip install --no-deps unsloth vllm\n",
    "# Install latest Hugging Face for Gemma-3!\n",
    "!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUFclG4wkf-z"
   },
   "outputs": [],
   "source": [
    "#@title Colab Extra Install { display-mode: \"form\" }\n",
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    !pip install --no-deps unsloth vllm\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    # Skip restarting message in Colab\n",
    "    import sys, re, requests; modules = list(sys.modules.keys())\n",
    "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "\n",
    "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
    "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
    "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
    "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
    "    !pip install -r vllm_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437,
     "referenced_widgets": [
      "b6b0600ebc8d4525b42dc47ef7f8d0c2",
      "55bd9228fcb244cabb9773cfd2f915ab",
      "d1c9ea78f91a46e39fb4964405156237",
      "f62fc5f74bc04990a4db2869278ab527",
      "f21618f019b34367a2d52491a789b0ae",
      "f897cd665f144faf8b4cf88028768f3a",
      "9934aa35fa4c46c6b837cb7d4b3d01c3",
      "7bf4a802ee584db5acb4d40edcf4d360",
      "be9bb4e7f4c24d7ab492b5ab73e26269",
      "aeb1d42e627e4828ba28d974b2967afe",
      "1795c4c2c89d4fc69a0ef2e7c08aee50",
      "eed3812268b74e608b2dbe6c7a83f2c0",
      "ea02bcb0b56b4f6b8cfd0671e83a06af",
      "323720c722d0458486f9855d77adb8f3",
      "ae365b483de847afaf12996d177036f8",
      "bbfa20422c5d4fd990534c96a27a366e",
      "b948ce1f240143f3a80e3bc5f8690e52",
      "1ec15247046646fe99da7abf34fca5fb",
      "24754bcc07bc4f0191d956d846993bf2",
      "925e6921bffd4455844049c4cccf162a",
      "b68ff22288d84cce9b5f66c09d02acbd",
      "62a10165a5bd4d2aa86ecc088664d46e",
      "8806609cecaf4af4b1b26980122eb0ca",
      "c6f364eb35344545a31f3a19e9a5e6f4",
      "1e90c9f9a59e44ba939e1f0d3dedb77a",
      "e76cc4aad50145c5ba6635b2be106408",
      "abc887f77c424d11a439bd13ceca5f31",
      "b2ab1aa549f34603934acb4b49bf55a8",
      "a47805941e7c470dbb22ef0b209de946",
      "df734a36f2b44267a6fc015e53cf70d2",
      "495bd620639442f0836831b37a2935e7",
      "43066d88fa5c471bb447f743fafe1d47",
      "3c71b24c08ab4bb78435e89af26b0fb4",
      "ad55dac691de41cfa379c9d68ac58543",
      "6b7a5fe7e1b943349aaa883a61ea4528",
      "f1693cf48c8e4c0d979bca3309d9f30f",
      "fc8c8a8c2c01420b92373f387c2c6203",
      "be618916dec248a38753dec2f93102f8",
      "5a5908493f7c4b7ea3e99f867cfbbc58",
      "fa8ff2de61e14cd19dd2cb9063757441",
      "2984086a26374e4f80fbfb2c3273205d",
      "bff39a74f57f4cc981c896ea77a3223e",
      "1d143c86f5884e2c81e20ea204cff263",
      "88b439dc539f4861ab9d7c99c7120623",
      "55fd0257b4b94b7cb9c3f05fb53a530a",
      "b7b06198c4a54ae180a04a37666680e2",
      "2a532a7331594f299dc439e13a365ec7",
      "97fc3b8472a04dcfa13c8fa21b09c1c3",
      "a402a2555c364047a7cb637a1aca8b05",
      "4b3131a802ff422a89fc427d313c9f46",
      "244e722b7ba64e93a2d0cdb44981e50c",
      "31f32acb4ffa4af2a19d33d8e989b528",
      "77e244b6af22436fbaa7ae1958471efa",
      "17ec033389f047f5a30fdf41fa9e543d",
      "387b041e028949d2a047dbf7d8baaef1",
      "0e2b8e659fa4402b8bbddb07507454ac",
      "1b55d628e017423fbdc64cbf6ea50e6d",
      "99b915bbb3d14cd89a5ae0c569a822ca",
      "f9569ef0e3ba44e3b330cbe959d6014c",
      "9df02290d96142cfac046eeb77676955",
      "6994c79748364b48b4f8d7e957e72163",
      "4e13a711461a40da9d8bc8a2373c3ceb",
      "9a50309928ec4eab83cde4ca865a37bb",
      "6ceff7a2b1b849bfafcee62b7421c972",
      "08763e73fcc349c6896b9cd154bae5e5",
      "96fd0f233c8f403591630823e14ec29c",
      "e9cc5415b83542689e43a4dd6e42eda8",
      "22e7cb81e1314d70a8bb7a2a317fdf27",
      "c7139adadec2477380db4b84b04e75ae",
      "97448afae361400f9aeeeaaae8f3d570",
      "dd3aee6103ce49e2ae058fa48a49ab83",
      "4320ee9867794dfaafd9ac79ce8fb3c8",
      "f35eb85886b042069c14eff4a791e36d",
      "70401f739b644544970ca15d715aab0d",
      "5576e65b619d4e03839f9f77813ac367",
      "2130f83d3b3a445597511529383084de",
      "c1aac7c004044d3d88f98a89f9fa751d"
     ]
    },
    "id": "5gwl_5LKDThj",
    "outputId": "7fc6da9b-d89f-4028-cd26-0cf422b53961"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "max_seq_length = 1024\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = max_seq_length, # Choose any for long context!\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNuwc5sJ2pYK",
    "outputId": "bf60b6fb-e9f1-4af8-fbfb-2542f646a8b9"
   },
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248,
     "referenced_widgets": [
      "ef713a3a359b47aa8b6915b33fc2bc27",
      "6d4e34dd69eb4b43afb18bb17ba011ae",
      "cbb2cdff385d4e0dac2d3abd70363fbe",
      "5ae859b821a540b7aa67e7f6aecb4c11",
      "76cf309520d14fc3b9fa154dbc4e5134",
      "73213aff64c94bc6ad68dd3b278aea65",
      "99433a20a6bb44dca48e78f12ae15988",
      "ddf88f22070047478ba790be3bb30359",
      "672e060b53b74b2396cbcbadd79f4492",
      "24928370f6754c82b77b37a5f702078d",
      "4ffa7eafa57f48848e8b3bdfdcc35865",
      "b663175e22c646c2a1279924f233c618",
      "0f4413c01d884c86a3a053edc79eca75",
      "497ce693a02f499283ca7f7ace660cbc",
      "b579b983c3a04a10aad4b59c37642a38",
      "c3c1cd315d92434ca8d91cb619f666a6",
      "265ba19532c14e238cce4b89ca8c493c",
      "81e691ca576748d8a51fc62aaf554037",
      "f61195c44013435faeab5441ce3e97ad",
      "70a2d85ad0fe4f36a3932c36f5fef55a",
      "68a71088a0d746f79202586fe4a86476",
      "2a346d3f4b834a13a068d850f6034fa2",
      "8616ded917dc4edd9fe85585ae72f643",
      "c237efa0aa754535833ce548f9ba2c21",
      "9abe71f47355475aae8737191effe7a9",
      "08c1ad0f3b084b83b981c03059b0ad79",
      "9a3c7480c78e4d38a3b956496287c1ff",
      "a9b586ad415b486d964db7284af3d700",
      "e843bde1ef6441e2be6783168862c8a7",
      "6977b174f54d46cc84517e4cc319a9b2",
      "2cd06c875daa419bb93abe9e3c86b390",
      "5f7bd6572f094f4c81a747b5fa9fc749",
      "a715a84f45ba4aff9f0593c78f80b5b6",
      "d77def2669274931a5ac89afb089e86e",
      "e52611fb5dbe440b8bafa052f1a3335f",
      "929c6d96e2324b7d87fa9e14a6d2ea83",
      "18dec4de9eef4ce68eb1f25096c6b48e",
      "bb416052bfcb406abe15ee57daa229fd",
      "3335a86132bc456bbb55abc2246b547e",
      "fa55e3688a3042eabe037eb2b116e952",
      "a869bf5120e643639e9add9e547a5c79",
      "0dabb46ac35b4c9dbb1183c80405c2e9",
      "d299ff6fb42b4b0791a88c453aca1325",
      "989fd4bea46f4085b25f5acbf729c03e",
      "7833b777eab74df0a06d60902c918a5e",
      "001618e5e4384e09ad20c95e013e7707",
      "a37be8821cee4685b623d7f492a7038b",
      "164a75373be24780b3d9bb7d134aacf6",
      "9203191b0e624eaba5d613e37981b83c",
      "9c4e8ada1d9a49629613db5606ab1cd2",
      "886b45c2a27e424f9d361ce1463a99bf",
      "3a959f91bd984b928123449400f1d173",
      "d7f155c5d06e43c6a3c7a650d80618e1",
      "70da3320cc0c4476970f177adde4336b",
      "58ff9335ed0944c89311ce94efbf6ac3"
     ]
    },
    "id": "GGbVyJ6zaThU",
    "outputId": "40231de2-3c2b-439f-c777-0f2ad3525f89"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\", split = \"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "yrVFdvhQaVgY",
    "outputId": "a838ea89-e768-4233-a014-d3467821b859"
   },
   "outputs": [],
   "source": [
    "dataset[0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "HfflR8Vqaay0",
    "outputId": "fe4fafbf-ce1f-4f30-ae8e-8a0d9a8d70cb"
   },
   "outputs": [],
   "source": [
    "dataset[0][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fcAg7QzhadlX",
    "outputId": "3aea4b02-d0dc-44b5-cc59-b753213bbd6e"
   },
   "outputs": [],
   "source": [
    "def extract_hash_answer(text):\n",
    "    if \"####\" not in text: return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "extract_hash_answer(dataset[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "_TJAMnMeagcE",
    "outputId": "0800ab87-be0f-4a62-ed0e-a4122b29f5e0"
   },
   "outputs": [],
   "source": [
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end   = \"<end_working_out>\"\n",
    "solution_start = \"<SOLUTION>\"\n",
    "solution_end = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210,
     "referenced_widgets": [
      "84f7a78f53ab40b99d19b311ead7df63",
      "0444b50e79c04373867dcd2694e41fb0",
      "957fe4d103004598b3ad5de0260238a7",
      "3b37af5fe901401aad20af08f4e098af",
      "f8c6984c80d5442dabf97a2dee4b6372",
      "272f74012c274409b33fde7878661aa9",
      "b79c9fd0baad44a7afe3042556a7aab8",
      "72fa3fd3578e4c7ab8f2bf6381e6721f",
      "c0c1957b93c6424693ad40b87259ef47",
      "4ecec2d5148843d79552765c059987f3",
      "44bcf342c91142c18bd3e9916996a9ed"
     ]
    },
    "id": "Yd9MoFDIaiuu",
    "outputId": "f83fd2b4-dcaf-4e8c-986f-7b8d36b25328"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: {\n",
    "    \"prompt\" : [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": x[\"question\"]},\n",
    "    ],\n",
    "    \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "})\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXM7Tu1xam0A"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\\\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ACLLCOhDap-_",
    "outputId": "8ba11ae4-af9a-4a1b-b624-35a625090687"
   },
   "outputs": [],
   "source": [
    "match_format.search(\n",
    "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
    "    \"<SOLUTION>2</SOLUTION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxJ01cBIaskQ"
   },
   "outputs": [],
   "source": [
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZamX11VzavkA"
   },
   "outputs": [],
   "source": [
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Count how many keywords are seen - we penalize if too many!\n",
    "        # If we see 1, then plus some points!\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7clstCzgay2-"
   },
   "outputs": [],
   "source": [
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        # Correct answer gets 3 points!\n",
    "        if guess == true_answer:\n",
    "            score += 3.0\n",
    "        # Match if spaces are seen\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 1.5\n",
    "        else:\n",
    "            # We also reward it if the answer is close via ratios!\n",
    "            # Ie if the answer is within some range, reward it!\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if   ratio >= 0.9 and ratio <= 1.1: score += 0.5\n",
    "                elif ratio >= 0.8 and ratio <= 1.2: score += 0.25\n",
    "                else: score -= 1.0 # Penalize wrong answers\n",
    "            except:\n",
    "                score -= 0.5 # Penalize\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFc9YHMla1VV",
    "outputId": "bd5ad2ab-4fb8-46e1-a7ba-7689cfc5ceb0"
   },
   "outputs": [],
   "source": [
    "match_numbers = re.compile(\n",
    "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "match_numbers.findall(\"<SOLUTION>  0.34  </SOLUTION>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pCcsgsWa4MI"
   },
   "outputs": [],
   "source": [
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    print('*'*20, f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            guess       = float(guess.strip())\n",
    "            scores.append(1.5 if guess == true_answer else 0.0)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaf588TSa6kf",
    "outputId": "bd6a1a18-e914-4c61-9fa4-6d4415e374dc"
   },
   "outputs": [],
   "source": [
    "max_prompt_length = 256\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_torch_fused\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_seq_length - max_prompt_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 50,\n",
    "    save_steps = 50,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "galtoa-da-z9",
    "outputId": "d898d6ae-b010-4ee7-ead7-c4a9fea39505"
   },
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jngXqsORbCwy",
    "outputId": "9ec2a6e7-7e59-429d-da36-2c36d8e4aed0"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199,
     "referenced_widgets": [
      "ab45da78bdc648cd8fe8cb469d5b37d8",
      "587c2b7c70c2417d8f692d48d1803017",
      "1eea735872104533ac48f30bf6d68a9f",
      "45820fef05fd43b49af5b9025d4f26e2",
      "aa62754252e644108ed3d6b49e577bf4",
      "91ae7dda990a48f5926c4b359bdc89e3",
      "eff5b109b43e4ed6855864aaeba94fa4",
      "fe77ecc6896e4db084be7efae5bde001",
      "485adce0255b41398046660a446eaa98",
      "57b5836697f943a6a53f40535e2a9820",
      "4b7141d28df3406ca7bc0a5916a84f96",
      "129fff574a4240b5b84074cb5602923b",
      "ac1fbec3d03d4317abccf257e1e2f2b2",
      "a02f94eb3c8a46beba4952554f83e905",
      "ff2b323f3261473b9d0135ab20a8cd39",
      "033462b36d244a57bdebbb258d1f2073",
      "3ee606ec0ae64b78908850eace83fed5",
      "2726ae4067bc43bc96206190ce50fd47",
      "e06d012d7efb4dc9b3a32c6e4dc0ba7a",
      "3e78abbb4e60482c83ec5d0fc5b0587e",
      "468edf243c1246e5851eff2db4f9e908",
      "51bf64bed77644549d7c94d59cb045f2",
      "f5ef1cddd4c24a7b9b8064ff73725c1b",
      "db33c36faa4a4923925c0bd727f0798e",
      "e384be5146914f34a0ba28513a290a2c",
      "e87101aa132948f1bbe42295b32da209",
      "ab17b9046c244a489ae3fe1820f5adb6",
      "4622515753324109819a2d71a3030025",
      "6d83ebb3c61f44f5b837ba23675fc876",
      "927750af5d1c4bbcbb49b2127b158714",
      "dd9c2087a60e45a1854d4d622d5588ae",
      "b733aad1a0404f5dbbdc729e70bdd839",
      "251e98cc57054c24881d767517b9c04f",
      "2cd60c1fde3e45578d6f1401a1fb6d6b",
      "ce8624fca3ba485baff077e330c25e77",
      "47b7a851d1f640acbefffe59d3b9329a",
      "91c9f0443ee24a028c4cc10de5fcc560",
      "0b1535e0201c473e92dab866a31829e5",
      "1e339f02cc6f4628ad189ad165e98e11",
      "f4839dd32e214a049fc52474bb387fef",
      "4bb698cf832648298eaa4fd04f380a79",
      "08eb39d1992146ca8f69557baf1758d6",
      "bc08e07f51614c6ca75b3f8b718a0f8b",
      "c7835a02caf2482d8d8fe91cf614c302"
     ]
    },
    "id": "yXJzFDwUm0ic",
    "outputId": "091a5dc0-d89c-4f1c-a25c-6d5e9f5bceed"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\"satyanayak/gemma-3-GRPO\", token = \"\")\n",
    "tokenizer.push_to_hub(\"satyanayak/gemma-3-GRPO\", token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIkcJFXVntiq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
